{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Network for optimal path finding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main objective of Q-training is to develope a policy **Ï€**  for navigating the maze successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os, sys, time, datetime, json, random, csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from numpy import genfromtxt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers.advanced_activations import ELU\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading of input data, setting of hyperparameters and other variables for Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wd = 'C:\\\\Users\\\\homeuser\\\\Documents\\\\MEJ\\\\City MSc\\\\09DISSERTATION\\\\Report\\\\PicturesRawData\\\\RL\\\\'\n",
    "visited_mark = 0.8  # Cells visited by the agent will be painted by gray 0.8\n",
    "agent_mark = 0.5      # The current agent cell will be painteg by gray 0.5\n",
    "LEFT = 0\n",
    "UP = 1\n",
    "RIGHT = 2\n",
    "DOWN = 3\n",
    "classes = np.arange(6)\n",
    "actions_dict = {\n",
    "    LEFT: 'left',\n",
    "    UP: 'up',\n",
    "    RIGHT: 'right',\n",
    "    DOWN: 'down',\n",
    "}# Actions dictionary\n",
    "num_actions = len(actions_dict)\n",
    "epsilon = 0.9# Exploration factor for Q-learning\n",
    "alpha = 1#Learning rate for Q-learning\n",
    "gamma = 0.9#Discount rate for Q-learning\n",
    "targetCell = (5,5)\n",
    "decrementRate = 0.1\n",
    "lr = 0.01#Learning rate for neural network\n",
    "envFileName = 'Environment.csv'\n",
    "rewardFileName = 'RewardComputation.csv'\n",
    "environment = pd.read_csv(wd+envFileName)\n",
    "rewardMatrix = pd.read_csv(wd+rewardFileName)\n",
    "env = np.array(environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reinforcement Learning **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qmaze class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Environment is a 2d Numpy array; agent = (row, col) initial agent position (defaults to (0,0))\n",
    "class Qmaze(object):\n",
    "    def __init__(self, maze, agent=(0,0)):\n",
    "        self._maze = np.array(maze)\n",
    "        nrows, ncols = self._maze.shape\n",
    "        self.target = (nrows-1, ncols-1)   # target cell\n",
    "        #self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] == 1.0]\n",
    "        self.free_cells = [(r,c) for r in range(nrows) for c in range(ncols) if self._maze[r,c] in classes]\n",
    "        self.free_cells.remove(self.target)\n",
    "        self.reset(agent)\n",
    "\n",
    "    def reset(self, agent):\n",
    "        self.agent = agent\n",
    "        self.maze = np.copy(self._maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        row, col = agent\n",
    "        self.maze[row, col] = agent_mark\n",
    "        self.state = (row, col, 'start')\n",
    "        self.min_reward = -0.5 * self.maze.size\n",
    "        self.total_reward = 0\n",
    "        self.visited = set()\n",
    "\n",
    "    def update_state(self, action):\n",
    "        nrows, ncols = self.maze.shape\n",
    "        nrow, ncol, nmode = agent_row, agent_col, mode = self.state\n",
    "\n",
    "        #if self.maze[agent_row, agent_col] > 0.0:\n",
    "        self.visited.add((agent_row, agent_col))  # mark visited cell\n",
    "\n",
    "        valid_actions = self.valid_actions()\n",
    "                \n",
    "        if not valid_actions:\n",
    "            nmode = 'blocked'\n",
    "        elif action in valid_actions:\n",
    "            nmode = 'valid'\n",
    "            if action == LEFT:\n",
    "                ncol -= 1\n",
    "            elif action == UP:\n",
    "                nrow -= 1\n",
    "            if action == RIGHT:\n",
    "                ncol += 1\n",
    "            elif action == DOWN:\n",
    "                nrow += 1\n",
    "        else:                  # invalid action, no change in agent position\n",
    "            mode = 'invalid'\n",
    "\n",
    "        self.state = (nrow, ncol, nmode)# new state\n",
    "\n",
    "        currClass = self.maze[agent_row, agent_col]  \n",
    "        nextClass = self.maze[nrow, ncol]      \n",
    "        return currClass,nextClass\n",
    "\n",
    "    def class_reward(self, sourceId, targetId):\n",
    "        temp = rewardMatrix[rewardMatrix['SourceID'] == sourceId]\n",
    "        reward = temp[temp['TargetID']==targetId]['Cost'].iloc[0]\n",
    "        return reward            \n",
    "\n",
    "    def get_reward(self):\n",
    "        agent_row, agent_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if agent_row == targetCell[0] and agent_col == targetCell[1]:\n",
    "            return 1.0\n",
    "        #if mode == 'blocked':\n",
    "        #    return self.min_reward - 1\n",
    "        if (agent_row, agent_col) in self.visited:\n",
    "            return -0.25\n",
    "        #if mode == 'invalid':\n",
    "        #    return -0.75\n",
    "        if mode == 'valid':\n",
    "            return -0.04\n",
    "\n",
    "    def act(self, action):\n",
    "        currentClass, targetClass = self.update_state(action)\n",
    "        reward = float(self.get_reward()+ self.class_reward(currentClass, targetClass))\n",
    "        self.total_reward = self.total_reward + alpha*reward\n",
    "        status = self.game_status()\n",
    "        envstate = self.observe()\n",
    "        return envstate, reward, status\n",
    "\n",
    "    def observe(self):\n",
    "        canvas = self.draw_env()\n",
    "        envstate = canvas.reshape((1, -1))\n",
    "        return envstate\n",
    "\n",
    "    def draw_env(self):\n",
    "        canvas = np.copy(self.maze)\n",
    "        nrows, ncols = self.maze.shape\n",
    "        # clear all visual marks\n",
    "        for r in range(nrows):\n",
    "            for c in range(ncols):\n",
    "                if canvas[r,c] > 0.0:\n",
    "                    canvas[r,c] = 1.0\n",
    "        # draw the agent\n",
    "        row, col, valid = self.state\n",
    "        canvas[row, col] = agent_mark\n",
    "        return canvas\n",
    "\n",
    "    def game_status(self):\n",
    "        agent_row, agent_col, mode = self.state\n",
    "        nrows, ncols = self.maze.shape\n",
    "        r,c=targetCell\n",
    "        if agent_row == r and agent_col == c:\n",
    "            return 'win'\n",
    "\n",
    "    def valid_actions(self, cell=None):\n",
    "        if cell is None:\n",
    "            row, col, mode = self.state\n",
    "        else:\n",
    "            row, col = cell\n",
    "        actions = [0, 1, 2, 3]\n",
    "        nrows, ncols = self.maze.shape\n",
    "        if row == 0:\n",
    "            actions.remove(1)\n",
    "        elif row == nrows-1:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col == 0:\n",
    "            actions.remove(0)\n",
    "        elif col == ncols-1:\n",
    "            actions.remove(2)\n",
    "\n",
    "        if row>0 and self.maze[row-1,col] == 0.0:\n",
    "            actions.remove(1)\n",
    "        if row<nrows-1 and self.maze[row+1,col] == 0.0:\n",
    "            actions.remove(3)\n",
    "\n",
    "        if col>0 and self.maze[row,col-1] == 0.0:\n",
    "            actions.remove(0)\n",
    "        if col<ncols-1 and self.maze[row,col+1] == 0.0:\n",
    "            actions.remove(2)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show(qmaze):\n",
    "    plt.grid('on')\n",
    "    nrows, ncols = qmaze.maze.shape\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(0.5, nrows, 1))\n",
    "    ax.set_yticks(np.arange(0.5, ncols, 1))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    canvas = np.copy(qmaze.maze)\n",
    "    for row,col in qmaze.visited:\n",
    "        canvas[row,col] = 0.6\n",
    "    agent_row, agent_col, _ = qmaze.state\n",
    "    canvas[agent_row, agent_col] = 0.3   # start cell\n",
    "    canvas[nrows-1, ncols-1] = 0.9 # target cell\n",
    "    img = plt.imshow(canvas, interpolation='none', cmap='jet')    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display of environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward= 2.96\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21d11a64ef0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAADuCAYAAADbValnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDEAiDEAiDEDyX/mKJLw2Eb/gAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21d0f99b358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze = Qmaze(env)\n",
    "canvas, reward, game_over = qmaze.act(DOWN)\n",
    "print(\"reward=\", reward)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21d11b06c18>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAADuCAYAAADbValnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbH4brEIAi50AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21d11a44dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze.act(DOWN)  # move down\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(RIGHT)  # move right\n",
    "qmaze.act(UP)  # move up\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that simulates state transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def play_game(model, qmaze, agent_cell):\n",
    "    qmaze.reset(agent_cell)\n",
    "    envstate = qmaze.observe()\n",
    "    while True:\n",
    "        prev_envstate = envstate\n",
    "        # get next action\n",
    "        q = model.predict(prev_envstate)\n",
    "        action = np.argmax(q[0])\n",
    "\n",
    "        # apply action, get rewards and new state\n",
    "        envstate, reward, game_status = qmaze.act(action)\n",
    "        if game_status == 'win':\n",
    "            return True\n",
    "        elif game_status == 'lose':\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Experience(object):\n",
    "    def __init__(self, model, max_memory=100, discount=0.95):\n",
    "        self.model = model\n",
    "        self.max_memory = max_memory\n",
    "        self.discount = discount\n",
    "        self.memory = list()\n",
    "        self.num_actions = model.output_shape[-1]\n",
    "\n",
    "    def remember(self, episode):\n",
    "        self.memory.append(episode)\n",
    "        if len(self.memory) > self.max_memory:\n",
    "            del self.memory[0]\n",
    "\n",
    "    def predict(self, envstate):\n",
    "        return self.model.predict(envstate)[0]\n",
    "\n",
    "    def get_data(self, alpha, data_size=10):\n",
    "        env_size = self.memory[0][0].shape[1]   # envstate 1d size (1st element of episode)\n",
    "        mem_size = len(self.memory)\n",
    "        data_size = min(mem_size, data_size)\n",
    "        inputs = np.zeros((data_size, env_size))\n",
    "        targets = np.zeros((data_size, self.num_actions))\n",
    "        for i, j in enumerate(np.random.choice(range(mem_size), data_size, replace=False)):\n",
    "            envstate, action, reward, envstate_next, game_over = self.memory[j]\n",
    "            inputs[i] = envstate\n",
    "            # There should be no target values for actions not taken.\n",
    "            targets[i] = self.predict(envstate)\n",
    "            # Q_sa = derived policy = max quality env/action = max_a' Q(s', a')\n",
    "            Q_sa = np.max(self.predict(envstate_next))\n",
    "            if game_over:\n",
    "                targets[i, action] = reward\n",
    "            else:\n",
    "                # reward + gamma * max_a' Q(s', a')\n",
    "                targets[i, action] = alpha * ((reward + self.discount * Q_sa)-reward)\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def qtrain(model, maze, n_epoch=10, max_memory=100, data_size=32, **opt):\n",
    "    global epsilon\n",
    "    global alpha\n",
    "    #weights_file = opt.get('weights_file', \"\")#Use pre-trained model by providing h5 file name to weights_file option\n",
    "    name = opt.get('name', 'model')\n",
    "    start_time = datetime.datetime.now()\n",
    "    #if weights_file:\n",
    "        #print(\"loading weights from file: %s\" % (weights_file,))\n",
    "        #model.load_weights(weights_file)\n",
    "\n",
    "    # Construct environment/game from numpy array: maze (see above)\n",
    "    qmaze = Qmaze(maze)\n",
    "\n",
    "    # Initialize experience replay object\n",
    "    experience = Experience(model, max_memory=max_memory, discount=gamma)\n",
    "\n",
    "    lossForAllEpochs = []\n",
    "    rewardForAllEpochs = []\n",
    "\n",
    "    for epoch in range(1,n_epoch+1):\n",
    "        print(\"Epoch ==> \",epoch)\n",
    "        lossOverEpisodes = []\n",
    "        rewardOverEpisodes = []\n",
    "        agent_cell = random.choice(qmaze.free_cells)\n",
    "        qmaze.reset(agent_cell)\n",
    "        game_over = False\n",
    "        if (epoch%5 == 0):\n",
    "            alpha = alpha - (decrementRate*alpha)\n",
    "            epsilon = epsilon - (decrementRate*epsilon)\n",
    "\n",
    "        # get initial envstate (1d flattened canvas)\n",
    "        envstate = qmaze.observe()\n",
    "\n",
    "        n_episodes = 0\n",
    "        while not game_over:\n",
    "            #currentRow, currentCol, mode = self.state\n",
    "            valid_actions = qmaze.valid_actions()\n",
    "            if not valid_actions: break\n",
    "            prev_envstate = envstate\n",
    "            # Get next action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = random.choice(valid_actions)\n",
    "            else:\n",
    "                action = np.argmax(experience.predict(prev_envstate))\n",
    "\n",
    "            # Apply action, get reward and new envstate\n",
    "            envstate, reward, game_status = qmaze.act(action)\n",
    "            if game_status == 'win':\n",
    "                #win_history.append(1)\n",
    "                game_over = True\n",
    "            else:\n",
    "                #win_history.append(0)#Newly added\n",
    "                game_over = False\n",
    "\n",
    "            # Store episode (experience)\n",
    "            episode = [prev_envstate, action, reward, envstate, game_over]\n",
    "            experience.remember(episode)\n",
    "            n_episodes += 1\n",
    "\n",
    "            # Train neural network model\n",
    "            inputs, targets = experience.get_data(alpha, data_size=data_size)\n",
    "            h = model.fit(\n",
    "                inputs,\n",
    "                targets,\n",
    "                epochs=8,\n",
    "                batch_size=16,\n",
    "                verbose=0,\n",
    "            )\n",
    "            loss = model.evaluate(inputs, targets, verbose=0)\n",
    "            lossOverEpisodes.append(loss)\n",
    "            rewardOverEpisodes.append(reward)\n",
    "   \n",
    "        dt = datetime.datetime.now() - start_time\n",
    "        t = format_time(dt.total_seconds())\n",
    "        template = \"Epoch: {:03d}/{:d} | LR: {:.4f} | Exploration rate: {:.4f} | Final reward: {:.4f} | Final loss: {:.4f} | Training time: {}\"\n",
    "        print(template.format(epoch, n_epoch-1, alpha, epsilon, reward, loss, t))\n",
    "\n",
    "        if game_status == 'win':\n",
    "            print(\"Reached target in: [\", n_episodes, \"] episodes in epoch: [\", epoch,\"]\")\n",
    "\n",
    "    lossForAllEpochs.append(lossOverEpisodes)\n",
    "    rewardForAllEpochs.append(rewardOverEpisodes)\n",
    "\n",
    "    h5file = name + \".h5\"# Save trained model weights and architecture\n",
    "    json_file = name + \".json\"\n",
    "    model.save_weights(h5file, overwrite=True)\n",
    "    with open(json_file, \"w\") as outfile:\n",
    "        json.dump(model.to_json(), outfile)\n",
    "    end_time = datetime.datetime.now()\n",
    "    dt = datetime.datetime.now() - start_time\n",
    "    seconds = dt.total_seconds()\n",
    "    t = format_time(seconds)\n",
    "    print('files: %s, %s' % (h5file, json_file))\n",
    "    print(\"n_epoch: %d, max_mem: %d, data: %d, time: %s\" % (epoch, max_memory, data_size, t))\n",
    "\n",
    "    return lossForAllEpochs\n",
    "\n",
    "def format_time(seconds):#Time formatting\n",
    "    if seconds < 400:\n",
    "        s = float(seconds)\n",
    "        return \"%.1f sec\" % (s,)\n",
    "    elif seconds < 4000:\n",
    "        m = seconds / 60.0\n",
    "        return \"%.2f min\" % (m,)\n",
    "    else:\n",
    "        h = seconds / 3600.0\n",
    "        return \"%.2f hr\" % (h,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN approach\n",
    "    1) Activation function is ELU (Exponential relu)\n",
    "    2) Optimizer is Adam\n",
    "    3) Loss function is MSE (Mean Squared Error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(maze):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(maze.size, input_shape=(maze.size,)))\n",
    "    model.add(ELU(lr))\n",
    "    model.add(Dense(maze.size))\n",
    "    model.add(ELU(lr))\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x21d11b37048>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAADuCAYAAADbValnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21d11a60c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qmaze = Qmaze(env)\n",
    "show(qmaze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch ==>  1\n",
      "Epoch: 001/7 | LR: 1.0000 | Exploration rate: 0.9000 | Final reward: 4.0000 | Final loss: 0.7557 | Training time: 0.7 sec\n",
      "Reached target in: [ 1 ] episodes in epoch: [ 1 ]\n",
      "Epoch ==>  2\n",
      "Epoch: 002/7 | LR: 1.0000 | Exploration rate: 0.9000 | Final reward: 3.5000 | Final loss: 0.0001 | Training time: 17.4 sec\n",
      "Reached target in: [ 402 ] episodes in epoch: [ 2 ]\n",
      "Epoch ==>  3\n",
      "Epoch: 003/7 | LR: 1.0000 | Exploration rate: 0.9000 | Final reward: 3.5000 | Final loss: 0.0523 | Training time: 60.9 sec\n",
      "Reached target in: [ 1019 ] episodes in epoch: [ 3 ]\n",
      "Epoch ==>  4\n",
      "Epoch: 004/7 | LR: 1.0000 | Exploration rate: 0.9000 | Final reward: 3.5000 | Final loss: 0.0005 | Training time: 96.3 sec\n",
      "Reached target in: [ 797 ] episodes in epoch: [ 4 ]\n",
      "Epoch ==>  5\n"
     ]
    }
   ],
   "source": [
    "model = build_model(env)\n",
    "trainingLoss = qtrain(model, env, 8, max_memory=1*env.size, data_size=32)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
