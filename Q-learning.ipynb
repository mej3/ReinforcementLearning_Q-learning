{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from random import randint\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD , Adam, RMSprop\n",
    "from keras.layers.advanced_activations import PReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load input data, set hyperparameters and other variables for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wd = 'C:\\\\Users\\\\homeuser\\\\Documents\\\\MEJ\\\\City MSc\\\\09DISSERTATION\\\\Coding\\\\RLcoding\\\\'\n",
    "wd = 'C:\\\\Users\\\\homeuser\\\\Documents\\\\MEJ\\\\City MSc\\\\09DISSERTATION\\\\Report\\\\PicturesRawData\\\\RL\\\\'\n",
    "os.chdir(wd)\n",
    "rewardFileName = 'RewardComputation.csv'\n",
    "envFileName = 'Environment.csv'\n",
    "rewardMatrix = pd.read_csv(wd+rewardFileName)\n",
    "environment = pd.read_csv(wd+envFileName)\n",
    "env = np.array(environment)\n",
    "learningAlpha = 0.9\n",
    "discountGamma = 0.9\n",
    "greedyEpsilon = 0.9\n",
    "greedyEpsilonRate = 0.1\n",
    "episodeConvergence = 1 # 1 = Not Converged , 0= Converged\n",
    "QMat_Old = np.zeros(shape=(5,5))# Initialise Q-Matrix Matrices\n",
    "QMat_New = np.zeros(shape=(5,5))\n",
    "Q_Old = 0\n",
    "Q_New = 0\n",
    "QfromEpisodes = pd.DataFrame(columns = ['Episode','QMatrixaverage','EpisodeTotalQ','Policy','PolicyLength','QvaluesForEpisode'])\n",
    "rewardStepChange = -5\n",
    "rewardCost = 0\n",
    "currentReward = 0\n",
    "endPos = (5,5)\n",
    "num_actions = 4\n",
    "nrows, ncols = env.shape\n",
    "target = (nrows-1, ncols-1)\n",
    "visited_mark = 0.8  # Cells visited by the agent will be painted by gray 0.8\n",
    "#agent_mark = 0.5      # The current agent cell will be painteg by gray 0.5\n",
    "actions_dict = {\n",
    "    0:'LEFT',\n",
    "    1: 'UP',\n",
    "    2: 'RIGHT',\n",
    "    3: 'DOWN',\n",
    "}# Actions dictionary\n",
    "actionDecode_dict = {\n",
    "    'LEFT': 0,\n",
    "    'UP': 1,\n",
    "    'RIGHT': 2,\n",
    "    'DOWN': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User-defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valid_actions(cell):\n",
    "    row, col = cell\n",
    "    actions = [0, 1, 2, 3]\n",
    "    nrows, ncols = env.shape\n",
    "    if row == 0:\n",
    "        actions.remove(1)\n",
    "    elif row == nrows-1:\n",
    "        actions.remove(3)\n",
    "\n",
    "    if col == 0:\n",
    "        actions.remove(0)\n",
    "    elif col == ncols-1:\n",
    "        actions.remove(2)\n",
    "    return actions\n",
    "        \n",
    "def act(currentPos, action):\n",
    "    [row,col] = currentPos\n",
    "    if(action == 'DOWN'):\n",
    "        row = row + 1\n",
    "    elif(action == 'UP'):\n",
    "        row = row - 1\n",
    "    elif(action == 'LEFT'):\n",
    "        col = col - 1\n",
    "    elif(action == 'RIGHT'):\n",
    "        col = col + 1\n",
    "    nxtState = env[row,col]\n",
    "    nxtPos = (row,col)\n",
    "    return nxtState, nxtPos\n",
    "\n",
    "def getReward(sourceId,target):\n",
    "    temp = rewardMatrix[rewardMatrix['SourceID'] == sourceId]\n",
    "    reward = temp[temp['TargetID']==target]['Cost'].iloc[0]\n",
    "    return reward\n",
    "    \n",
    "def selectGreedyTargetsReward (nextStateReward):\n",
    "    item = nextStateReward.index(max(nextStateReward))    \n",
    "    return nextPos[item],nextState[item],max(nextStateReward),nextAction[item]\n",
    "    \n",
    "def selectRandomTargetsReward (nextStateReward):\n",
    "    item = randint(0,len(nextStateReward)-1)\n",
    "    return nextPos[item],nextState[item],nextStateReward[item],nextAction[item]\n",
    "    \n",
    "def calculatenoOfEpochs():\n",
    "    noOfEpochs = int((greedyEpsilon- greedyEpsilonRate)/greedyEpsilonRate) + 1\n",
    "    return noOfEpochs\n",
    "    \n",
    "def exploreAndExploit(Epsilon):#Explore : 1 and Exploit : 0\n",
    "    if random.uniform(0, 1) <= Epsilon:\n",
    "        return 1\n",
    "    return 0\n",
    "    \n",
    "def bellmanEqn(currentStateReward,Q_Old,target):   # calculate maximum Q Value from Target to Targets of Target\n",
    "    targetOfTargetList = rewardMatrix[rewardMatrix['SourceID'] == target]['TargetID'].tolist()\n",
    "    nextStateValidActions = valid_actions(targetPos)\n",
    "    validTargetOfTargetList = list(set(targetOfTargetList) & set(nextStateValidActions))    \n",
    "    Q_temp = np.zeros(shape=(len(validTargetOfTargetList),1))#Fix needed to check for visited\n",
    "    for i in range(0,len(validTargetOfTargetList)):\n",
    "        Q_temp[i] = QMat_Old[target, i]\n",
    "    Q_max =  max(Q_temp)\n",
    "    Qnew = Q_Old + learningAlpha * ((currentStateReward + (discountGamma * Q_max))-Q_Old)\n",
    "    return(Qnew) # Qnew = Qold + alpha[{reward + (gamma * Q_max)}-Qold]\n",
    "    \n",
    "def build_model(env, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(env.size, input_shape=(env.size,)))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(env.size))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dense(num_actions))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-learning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best policy ==>  4    [2, 1, 1, 1, 0, 1, 0, 1, 0, 2, 0, 1, 1, 1, 1, ...\n",
      "Name: Policy, dtype: object\n"
     ]
    }
   ],
   "source": [
    "visited = set()\n",
    "noOfEpochs = calculatenoOfEpochs()\n",
    "#noOfEpochs = 5\n",
    "QvalueForAllEpisodes = pd.DataFrame(index=range(noOfEpochs))\n",
    "for i in range(5):\n",
    "    #print('Episode: ==> ', i)\n",
    "    stateCount = 0    \n",
    "    episodeQvalue = []\n",
    "    policyPie = [env[0,0]]\n",
    "    Qtrain_Reward = []\n",
    "    currentPos = (0,0)\n",
    "    Qtrain_Done = []\n",
    "    Qtrain_Action = []\n",
    "    episodeConvergence = 1\n",
    "    QvalueForAllEpisodes_row = 0\n",
    "    while(episodeConvergence):\n",
    "        \n",
    "        row, col = currentPos\n",
    "        currentState = env[currentPos[0],currentPos[1]]\n",
    "        validActions = valid_actions(currentPos)\n",
    "        \n",
    "        nextAction = []\n",
    "        nextState = []\n",
    "        nextStateReward = []\n",
    "        nextPos = []\n",
    "        for i,each in enumerate(validActions):\n",
    "            nextAction.append(actions_dict[each])\n",
    "            [state, pos] = act(currentPos,nextAction[i])\n",
    "            nextState.append(state)\n",
    "            nextPos.append(pos)\n",
    "            nextStateReward.append(getReward(currentState,nextState[i]))\n",
    "        \n",
    "        explore = exploreAndExploit(greedyEpsilon)# Decide explore or exploit\n",
    "        if explore:\n",
    "            [targetPos, targetState, targetStateReward, targetAction] = selectRandomTargetsReward(nextStateReward)\n",
    "            #print('random')\n",
    "        else:\n",
    "            [targetPos, targetState, targetStateReward, targetAction] = selectGreedyTargetsReward(nextStateReward)\n",
    "            #print('deterministic')\n",
    "        policyPie.append(targetState)\n",
    "        Qtrain_Action.append(actionDecode_dict[targetAction])\n",
    "        \n",
    "        Q_Old = QMat_Old[currentState,targetState]# Get Q Value of Current State and Target\n",
    "        if targetPos in visited:\n",
    "            visitPenalty = -20\n",
    "        else:\n",
    "            visitPenalty = 0\n",
    "        currentStateReward = targetStateReward + rewardStepChange + visitPenalty\n",
    "        \n",
    "        Q_New = bellmanEqn(currentStateReward,Q_Old,targetState)# Calculate New Q-Value based on Old QValue and Reward  \n",
    "        Qtrain_Reward.append(Q_New[0])\n",
    "        \n",
    "        QMat_New[currentState, targetState] = Q_New#update the New Q-Value in Q New matrix\n",
    "        QvalueForAllEpisodes[i,QvalueForAllEpisodes_row] = float(Q_New)\n",
    "        QvalueForAllEpisodes_row += 1\n",
    "        episodeQvalue.append(float(Q_New))\n",
    "        stateCount = stateCount + 1\n",
    "        visited.add(targetPos)\n",
    "        currentPos = targetPos#Update Current state as Target state Because in next intration it will act as source\n",
    "        if(currentPos == endPos):            \n",
    "            episodeConvergence = 0\n",
    "            Qtrain_Done.append(1)\n",
    "            #print(policyPie)\n",
    "            #QMatrixaverage = (QMat_New.sum())/(rewardMatrix.shape[0])\n",
    "            #EpisodeAvgQ = np.mean(episodeQvalue)\n",
    "            #QfromEpisodes.loc[len(QfromEpisodes)] = [i,QMatrixaverage,EpisodeAvgQ,policyPie]            \n",
    "        else:\n",
    "            Qtrain_Done.append(0)\n",
    "            \n",
    "    QMatrixaverage = (QMat_New.sum())/(rewardMatrix.shape[0])\n",
    "    #EpisodeAvgQ = np.mean(episodeQvalue)\n",
    "    EpisodeSumQ = np.sum(episodeQvalue)\n",
    "    #QfromEpisodes.ix([i,QMatrixaverage,EpisodeAvgQ,policyPie])\n",
    "    #QfromEpisodes.loc[len(QfromEpisodes)] = [i,QMatrixaverage,EpisodeAvgQ,policyPie,len(policyPie)]\n",
    "    QfromEpisodes.loc[len(QfromEpisodes)] = [i,QMatrixaverage,EpisodeSumQ,policyPie,len(policyPie),Qtrain_Reward]\n",
    "\n",
    "bestReward = QfromEpisodes['EpisodeTotalQ'].max()\n",
    "bestPolicy = QfromEpisodes[QfromEpisodes['EpisodeTotalQ']==bestReward]\n",
    "bestQvalues = QfromEpisodes[QfromEpisodes['EpisodeTotalQ']==bestReward]['QvaluesForEpisode']\n",
    "\n",
    "Qtrain = [policyPie[1:], Qtrain_Reward, Qtrain_Done, Qtrain_Action]\n",
    "[a,b,c,d] = Qtrain\n",
    "\n",
    "QfromEpisodes.to_csv('QlearningEpisodes1.csv')\n",
    "bestQvalues.to_csv('bestQ.csv')\n",
    "\n",
    "print(\"Best policy ==> \", bestPolicy['Policy'])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
